import os
import sys
import cv2
import numpy as np
import mediapipe as mp
from tqdm import tqdm

# ê²½ë¡œ ì„¤ì •
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(ROOT)

# ---------------------------------------------------------
# [ì„¤ì •] í…ŒìŠ¤íŠ¸í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ë„£ìœ¼ì„¸ìš”!
# ---------------------------------------------------------
VIDEO_PATH = r"C:\Users\SUNWOO\Desktop\AI\AI_ML_UnityProject\AI_ML_Python_Final\final_project\data\raw_videos\í˜„ì‹œê°_1ìœ„_ëŒ„ìŠ¤ì±Œë¦°ì§€_#ë‹¤ì˜_#body.mp4"
# ---------------------------------------------------------

# ì €ì¥ë  ìœ„ì¹˜: ë°”ë¡œ í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•˜ë„ë¡ processed/test í´ë”ë¡œ ì§€ì •
OUTPUT_DIR = os.path.join(ROOT, "data", "processed", "test")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# MediaPipe ì„¤ì •
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=2,
    smooth_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

def _normalize(v, eps=1e-8):
    n = np.linalg.norm(v, axis=-1, keepdims=True)
    return v / (n + eps)

def process_pipeline(raw_seq):
    """
    Raw ë°ì´í„°(T, 33, 3)ë¥¼ ë°›ì•„ì„œ ëª¨ë¸ ì…ë ¥ìš©(Pose, Traj)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    """
    # 1. Smoothing (ê°„ë‹¨í•œ ì´ë™í‰ê· )
    T = raw_seq.shape[0]
    processed = raw_seq.copy()
    window = 5
    if T > window:
        kernel = np.ones(window)/window
        for i in range(33):
            for j in range(3):
                processed[:, i, j] = np.convolve(processed[:, i, j], kernel, mode='same')
    
    # 2. Body Frame Transform (Local ë³€í™˜ + Trajectory ì¶”ì¶œ)
    LHIP, RHIP = 23, 24
    LSHO, RSHO = 11, 12
    
    hip_center = np.mean(processed[:, [LHIP, RHIP], :], axis=1)
    sho_center = np.mean(processed[:, [LSHO, RSHO], :], axis=1)
    
    x_axis = _normalize(processed[:, LHIP, :] - processed[:, RHIP, :])
    y_axis_raw = _normalize(sho_center - hip_center)
    z_axis = _normalize(np.cross(x_axis, y_axis_raw))
    z_axis[:, 1] = 0 # Roll ì œê±°
    z_axis = _normalize(z_axis)
    y_axis = _normalize(np.cross(z_axis, x_axis))
    
    # íšŒì „ í–‰ë ¬
    R = np.stack([x_axis, y_axis, z_axis], axis=1) # (T, 3, 3)
    
    # Local Pose ë³€í™˜
    p = processed - hip_center[:, None, :]
    local_seq = np.einsum("tji,tbj->tbi", R, p)
    
    # Trajectory ìƒì„± (x, y, z, rotation_y)
    azimuth = np.arctan2(z_axis[:, 0], z_axis[:, 2])
    traj = np.zeros((T, 4), dtype=np.float32)
    traj[:, :3] = hip_center
    traj[:, 3] = azimuth
    
    # 3. Scale Normalization
    torso_len = np.mean(np.linalg.norm(sho_center - hip_center, axis=1))
    scale = float(torso_len) if torso_len > 1e-8 else 1.0
    
    local_seq /= scale
    traj[:, :3] /= scale # ìœ„ì¹˜ë§Œ ë‚˜ëˆ”
    
    # 4. Height Correction (ë°”ë‹¥ì  0ìœ¼ë¡œ ë§ì¶”ê¸°)
    LFOOT, RFOOT = 31, 32
    all_feet_y = np.concatenate([local_seq[:, LFOOT, 1], local_seq[:, RFOOT, 1]])
    min_ground = np.percentile(all_feet_y, 1) # í•˜ìœ„ 1%
    local_seq[:, :, 1] -= min_ground
    
    return local_seq.astype(np.float32), traj.astype(np.float32)

def main():
    if not os.path.exists(VIDEO_PATH):
        print(f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VIDEO_PATH}")
        return

    cap = cv2.VideoCapture(VIDEO_PATH)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"ğŸ¬ ì˜ìƒ ë¡œë“œë¨: {os.path.basename(VIDEO_PATH)} ({total_frames} frames)")

    raw_points = []
    
    # 1. MediaPipe ì¶”ì¶œ
    pbar = tqdm(total=total_frames, desc="Extracting Raw Keypoints")
    while True:
        ret, frame = cap.read()
        if not ret: break
        
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        res = pose.process(frame_rgb)
        
        if res.pose_world_landmarks:
            # (33, 3) ë°°ì—´ ë§Œë“¤ê¸°
            pts = [[lm.x, -lm.y, lm.z] for lm in res.pose_world_landmarks.landmark]
            raw_points.append(pts)
        pbar.update(1)
    
    cap.release()
    pbar.close()
    
    if not raw_points:
        print("âŒ í¬ì¦ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    raw_np = np.array(raw_points, dtype=np.float32) # (T, 33, 3)
    print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {raw_np.shape}")

    # 2. ì „ì²˜ë¦¬ (Smoothing -> Local -> Scale -> Grounding)
    print("âš™ï¸ ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘ (Processing)...")
    final_pose, final_traj = process_pipeline(raw_np)
    
    # 3. ì €ì¥ (processed/test í´ë”ì— ë°”ë¡œ ì €ì¥!)
    base_name = os.path.splitext(os.path.basename(VIDEO_PATH))[0]
    
    pose_path = os.path.join(OUTPUT_DIR, f"{base_name}_pose.npy")
    traj_path = os.path.join(OUTPUT_DIR, f"{base_name}_trajectory.npy")
    
    np.save(pose_path, final_pose)
    np.save(traj_path, final_traj)
    
    print("\nğŸ‰ [ì™„ë£Œ] í…ŒìŠ¤íŠ¸ ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤!")
    print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}")
    print(f"ğŸ‘‰ ì´ì œ 'src/test.py'ë¥¼ ë°”ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!")

if __name__ == "__main__":
    main()